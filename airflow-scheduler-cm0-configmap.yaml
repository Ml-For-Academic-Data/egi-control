apiVersion: v1
data:
  group_cleaning.py: |-
    # from airflow.utils.task_group import TaskGroup
    # from airflow.operators.python import PythonOperator

    # def build_cleaning_group(dag):

    #     with TaskGroup("cleaning_pipeline", tooltip="Pipeline de limpieza de datos") as group:

    #         task_1 = PythonOperator(
    #             task_id="handle_missing_values",
    #             python_callable=handle_missing_values,
    #             provide_context=True,
    #             dag=dag
    #         )

    #         task_2 = PythonOperator(
    #             task_id="remove_duplicate_rows",
    #             python_callable=remove_duplicate_rows,
    #             provide_context=True,
    #             dag=dag
    #         )

    #         task_3 = PythonOperator(
    #             task_id="filter_numeric_outliers",
    #             python_callable=filter_numeric_outliers,
    #             provide_context=True,
    #             dag=dag
    #         )

    #         task_4 = PythonOperator(
    #             task_id="create_derived_features",
    #             python_callable=create_derived_features,
    #             provide_context=True,
    #             dag=dag
    #         )

    #         task_1 >> task_2 >> task_3 >> task_4

    #     return group
  mlops_dag.py: |-
    import sys
    import os
    import pandas as pd
    from datetime import datetime
    from airflow import DAG
    from airflow.operators.python import PythonOperator
    from airflow.operators.dummy import DummyOperator

    # Asegúrate de que Python pueda encontrar módulos personalizados (si usas más adelante)
    SRC_PATH = "/opt/airflow/src"
    if SRC_PATH not in sys.path:
        sys.path.insert(0, SRC_PATH)

    # Definición de rutas globales
    RAW_DATA_PATH = "/opt/airflow/data/raw/dataset.csv"
    TEMP_DIR = "/opt/airflow/data/temp"
    PROCESSED_DIR = "/opt/airflow/data/processed"

    # Crear directorios si no existen
    os.makedirs(TEMP_DIR, exist_ok=True)
    os.makedirs(PROCESSED_DIR, exist_ok=True)

    # Funciones reutilizables del pipeline
    def handle_missing_values(df):
        """
        Maneja valores faltantes:
        - Categóricas: 'Unknown'
        - Numéricas: mediana
        """
        print("Manejando valores faltantes...")
        for col in df.columns:
            if df[col].isnull().sum() > 0:
                if df[col].dtype == "object":
                    df[col] = df[col].fillna("Unknown")
                else:
                    df[col] = df[col].fillna(df[col].median())
        return df


    def remove_duplicate_rows(df):
        """
        Elimina filas duplicadas completas.
        """
        print("Eliminando filas duplicadas...")
        return df.drop_duplicates()


    def filter_numeric_outliers(df, threshold=3):
        """
        Filtra outliers usando Z-score.
        """
        print("Filtrando outliers numéricos...")
        numeric_cols = df.select_dtypes(include='number').columns
        z_scores = (df[numeric_cols] - df[numeric_cols].mean()) / df[numeric_cols].std()
        filtered_df = df[(z_scores.abs() < threshold).all(axis=1)]
        return filtered_df


    def create_derived_features(df):
        """
        Crea nuevas características derivadas.
        Ejemplo: suma de columnas numéricas como feature adicional.
        """
        print("Creando características derivadas...")
        numeric_cols = df.select_dtypes(include='number').columns.tolist()
        df['total_score'] = df[numeric_cols].sum(axis=1)
        return df


    # Tareas del pipeline
    def load_data_task():
        """Carga los datos iniciales"""
        df = pd.read_csv(RAW_DATA_PATH)
        df.to_csv(f"{TEMP_DIR}/dataset_temp.csv", index=False)
        print(f"Datos cargados: {df.shape}")


    def handle_missing_task():
        """Maneja valores faltantes"""
        df = pd.read_csv(f"{TEMP_DIR}/dataset_temp.csv")
        df_cleaned = handle_missing_values(df)
        df_cleaned.to_csv(f"{TEMP_DIR}/step1_missing_handled.csv", index=False)
        print(f"Valores faltantes manejados: {df_cleaned.shape}")


    def remove_duplicates_task():
        """Remueve duplicados"""
        df = pd.read_csv(f"{TEMP_DIR}/step1_missing_handled.csv")
        df_cleaned = remove_duplicate_rows(df)
        df_cleaned.to_csv(f"{TEMP_DIR}/step2_duplicates_removed.csv", index=False)
        print(f"Duplicados removidos: {df_cleaned.shape}")


    def filter_outliers_task():
        """Filtra outliers"""
        df = pd.read_csv(f"{TEMP_DIR}/step1_missing_handled.csv")  # Desde step1
        df_cleaned = filter_numeric_outliers(df)
        df_cleaned.to_csv(f"{TEMP_DIR}/step2b_outliers_filtered.csv", index=False)
        print(f"Outliers filtrados: {df_cleaned.shape}")


    def create_features_task():
        """Crea features derivadas combinando ambos paths"""
        df1 = pd.read_csv(f"{TEMP_DIR}/step2_duplicates_removed.csv")
        df2 = pd.read_csv(f"{TEMP_DIR}/step2b_outliers_filtered.csv")

        # Usar el que tenga más filas (ejemplo simple)
        df_to_use = df1 if len(df1) > len(df2) else df2

        df_final = create_derived_features(df_to_use)
        df_final.to_csv(f"{PROCESSED_DIR}/cleaned_data.csv", index=False)
        print(f"Features creadas: {df_final.shape}")


    def generate_report_task():
        """Genera un reporte simple"""
        df = pd.read_csv(f"{PROCESSED_DIR}/cleaned_data.csv")

        report = f"""
    REPORTE DE LIMPIEZA DE DATOS
    ============================
    Total de filas: {len(df)}
    Total de columnas: {len(df.columns)}
    Valores nulos restantes: {df.isnull().sum().sum()}

    Columnas: {', '.join(df.columns.tolist())}
    """

        with open(f"{PROCESSED_DIR}/cleaning_report.txt", "w") as f:
            f.write(report)

        print("Reporte generado")


    # Configuración del DAG
    default_args = {
        "owner": "airflow",
        "start_date": datetime(2025, 1, 1),
    }

    with DAG(
        dag_id="mlops_dropout_prediction",
        default_args=default_args,
        schedule_interval="@daily",
        catchup=False,
    ) as dag:

        start = DummyOperator(task_id="start")

        load_data = PythonOperator(
            task_id="load_data",
            python_callable=load_data_task
        )

        handle_missing = PythonOperator(
            task_id="handle_missing_values",
            python_callable=handle_missing_task
        )

        # Dos ramas paralelas
        remove_duplicates = PythonOperator(
            task_id="remove_duplicates",
            python_callable=remove_duplicates_task
        )

        filter_outliers = PythonOperator(
            task_id="filter_outliers",
            python_callable=filter_outliers_task
        )

        # Unión en create_features
        create_features = PythonOperator(
            task_id="create_features",
            python_callable=create_features_task
        )

        # Tarea final de reporte
        generate_report = PythonOperator(
            task_id="generate_report",
            python_callable=generate_report_task
        )

        end = DummyOperator(task_id="end")

        # Flujo del DAG con ramificación
        start >> load_data >> handle_missing
        handle_missing >> remove_duplicates >> create_features
        handle_missing >> filter_outliers >> create_features
        create_features >> generate_report >> end
kind: ConfigMap
metadata:
  labels:
    io.kompose.service: airflow-scheduler
  name: airflow-scheduler-cm0
